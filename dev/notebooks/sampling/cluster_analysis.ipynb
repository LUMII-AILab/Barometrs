{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import hdbscan\n",
    "from sqlalchemy import func, Date\n",
    "from db import models\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import random\n",
    "\n",
    "# Database configuration\n",
    "SQLALCHEMY_DATABASE_URL = \"postgresql://barometrs:password@127.0.0.1:5433/barometrs\"\n",
    "\n",
    "engine = create_engine(SQLALCHEMY_DATABASE_URL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ],
   "id": "33e433325d1494ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_article_clusters():\n",
    "    session.rollback()\n",
    "\n",
    "    start_date = '2020-01-01'\n",
    "    article_language = 'lv'\n",
    "\n",
    "    query = session.query(\n",
    "        models.RawArticle.article_id.label('id'),\n",
    "        models.RawArticle.headline.label('article_title'),\n",
    "        models.RawArticle.embedding.label('embedding'),\n",
    "    ).join(\n",
    "        models.RawArticle.predicted_comments  # Join with comments\n",
    "    ).filter(\n",
    "        func.cast(models.RawArticle.pub_timestamp, Date) >= start_date,\n",
    "        models.RawArticle.embedding != None,\n",
    "        models.RawArticle.headline_lang == article_language,\n",
    "        models.PredictedComment.text_lang == article_language,\n",
    "    ).group_by(\n",
    "        models.RawArticle.article_id,\n",
    "        models.RawArticle.headline,\n",
    "        models.RawArticle.embedding\n",
    "    ).having(\n",
    "        func.count(models.PredictedComment.id) >= 10,\n",
    "    )\n",
    "\n",
    "    results = query.all()\n",
    "\n",
    "    print('Number of articles: ' + str(len(results)))\n",
    "\n",
    "    if not results:\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(results, columns=['id', 'article_title', 'embedding'])\n",
    "    articles_df = df[['id', 'article_title', 'embedding']].drop_duplicates(subset=['id'])\n",
    "    embeddings = np.array(articles_df['embedding'].tolist())\n",
    "\n",
    "    # Reduce dimensionality for more efficient clustering\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        random_state=42,\n",
    "        method='barnes_hut',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=100,\n",
    "        min_samples=100,\n",
    "        metric='euclidean',\n",
    "        algorithm='boruvka_kdtree',\n",
    "        core_dist_n_jobs=-1,\n",
    "        cluster_selection_method='leaf'\n",
    "    )\n",
    "    clusters = clusterer.fit_predict(embeddings_2d)  # Cluster on the 2D projection\n",
    "\n",
    "    # Add cluster information to articles dataframe\n",
    "    articles_df['cluster'] = clusters\n",
    "\n",
    "    return articles_df\n",
    "\n",
    "clustered_articles = get_article_clusters()\n",
    "clustered_articles"
   ],
   "id": "1d039df45fc8766c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = clustered_articles.copy()\n",
    "\n",
    "# drop -1 cluster\n",
    "df = df[df['cluster'] != -1]\n",
    "\n",
    "\n",
    "# Extract embeddings from dataframe - they should already be stored as lists or arrays\n",
    "embeddings = np.array(df['embedding'].tolist())\n",
    "\n",
    "# Generate t-SNE embeddings with the same parameters you used for clustering\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    random_state=42,\n",
    "    method='barnes_hut',\n",
    "    n_jobs=-1\n",
    ")\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Add the t-SNE coordinates to the dataframe\n",
    "df['tsne_1'] = embeddings_2d[:, 0]\n",
    "df['tsne_2'] = embeddings_2d[:, 1]\n",
    "\n",
    "# Convert cluster to string for better visualization\n",
    "df['cluster_str'] = df['cluster'].astype(str)\n",
    "\n",
    "# Create an interactive scatter plot with t-SNE dimensions\n",
    "fig_tsne = px.scatter(\n",
    "    df,\n",
    "    x='tsne_1',\n",
    "    y='tsne_2',\n",
    "    color='cluster_str',\n",
    "    hover_name='article_title',\n",
    "    hover_data=['id'],\n",
    "    title='Article Clusters with t-SNE',\n",
    "    labels={'tsne_1': 't-SNE Component 1',\n",
    "            'tsne_2': 't-SNE Component 2'},\n",
    "    opacity=0.7,\n",
    "    size_max=10\n",
    ")\n",
    "\n",
    "# Improve layout\n",
    "fig_tsne.update_layout(\n",
    "    legend_title_text='Cluster',\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Analyze cluster statistics\n",
    "cluster_stats = df.groupby('cluster').agg({\n",
    "    'id': 'count'\n",
    "}).rename(columns={'id': 'article_count'}).sort_values('article_count', ascending=False)\n",
    "\n",
    "# Create a bar chart for cluster sizes\n",
    "fig_bar = px.bar(\n",
    "    cluster_stats.reset_index(),\n",
    "    x='cluster',\n",
    "    y='article_count',\n",
    "    title='Articles per Cluster',\n",
    "    labels={'cluster': 'Cluster', 'article_count': 'Number of Articles'},\n",
    "    color='cluster',\n",
    "    text='article_count'\n",
    ")\n",
    "\n",
    "fig_bar.update_traces(textposition='outside')\n",
    "\n",
    "# Function to get top articles from each cluster\n",
    "def get_cluster_samples(df, n=5):\n",
    "    samples = pd.DataFrame()\n",
    "    for cluster in sorted(df['cluster'].unique()):\n",
    "        cluster_df = df[df['cluster'] == cluster]\n",
    "        if len(cluster_df) > 0:\n",
    "            sample = cluster_df.sample(min(n, len(cluster_df)))\n",
    "            samples = pd.concat([samples, sample])\n",
    "    return samples\n",
    "\n",
    "cluster_samples = get_cluster_samples(df)\n",
    "\n",
    "# Interactive function to explore specific clusters\n",
    "def explore_cluster(df, cluster_id):\n",
    "    cluster_df = df[df['cluster'] == cluster_id]\n",
    "    print(f\"Cluster {cluster_id} has {len(cluster_df)} articles\")\n",
    "\n",
    "    # Create a scatter plot for this cluster\n",
    "    fig = px.scatter(\n",
    "        cluster_df,\n",
    "        x='tsne_1',\n",
    "        y='tsne_2',\n",
    "        hover_name='article_title',\n",
    "        hover_data=['id'],\n",
    "        title=f'Articles in Cluster {cluster_id}',\n",
    "        labels={'tsne_1': 't-SNE Component 1',\n",
    "                'tsne_2': 't-SNE Component 2'}\n",
    "    )\n",
    "\n",
    "    # Add a table with sample articles from this cluster\n",
    "    print(\"\\nSample articles from this cluster:\")\n",
    "    display(cluster_df[['id', 'article_title']].head(10))\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Create a density map visualization to understand cluster density\n",
    "fig_density = px.density_contour(\n",
    "    df,\n",
    "    x='tsne_1',\n",
    "    y='tsne_2',\n",
    "    title='Density Map of Articles in Embedding Space'\n",
    ")\n",
    "fig_density.update_traces(contours_coloring=\"fill\", contours_showlabels=True)\n",
    "\n",
    "# Create a 3D visualization using t-SNE for 2 dimensions and cluster as third dimension\n",
    "fig_3d = px.scatter_3d(\n",
    "    df,\n",
    "    x='tsne_1',\n",
    "    y='tsne_2',\n",
    "    z='cluster',\n",
    "    color='cluster_str',\n",
    "    hover_name='article_title',\n",
    "    title='3D Visualization of Clusters',\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "def identify_cluster_boundaries(df):\n",
    "    # Create a new dataframe with t-SNE coordinates and cluster info\n",
    "    cluster_data = []\n",
    "\n",
    "    for cluster in sorted(df['cluster'].unique()):\n",
    "        if cluster == -1:  # Skip noise points\n",
    "            continue\n",
    "\n",
    "        cluster_df = df[df['cluster'] == cluster]\n",
    "\n",
    "        # Calculate centroid for this cluster\n",
    "        centroid_x = cluster_df['tsne_1'].mean()\n",
    "        centroid_y = cluster_df['tsne_2'].mean()\n",
    "\n",
    "        # Calculate distance from each point to centroid\n",
    "        for _, row in cluster_df.iterrows():\n",
    "            distance = np.sqrt((row['tsne_1'] - centroid_x)**2 + (row['tsne_2'] - centroid_y)**2)\n",
    "\n",
    "            cluster_data.append({\n",
    "                'id': row['id'],\n",
    "                'article_title': row['article_title'],\n",
    "                'cluster': row['cluster'],\n",
    "                'distance_to_centroid': distance\n",
    "            })\n",
    "\n",
    "    boundary_df = pd.DataFrame(cluster_data)\n",
    "\n",
    "    # Determine core vs boundary points (simplified approach)\n",
    "    # We'll consider points in the 25th percentile as core, and the rest as boundary\n",
    "    boundary_df['point_type'] = 'boundary'\n",
    "\n",
    "    for cluster in boundary_df['cluster'].unique():\n",
    "        cluster_distances = boundary_df[boundary_df['cluster'] == cluster]['distance_to_centroid']\n",
    "        threshold = cluster_distances.quantile(0.25)\n",
    "        boundary_df.loc[(boundary_df['cluster'] == cluster) &\n",
    "                        (boundary_df['distance_to_centroid'] <= threshold), 'point_type'] = 'core'\n",
    "\n",
    "    return boundary_df\n",
    "\n",
    "# Create the core/boundary analysis\n",
    "boundary_df = identify_cluster_boundaries(df)\n",
    "\n",
    "# Visualize core vs boundary points\n",
    "fig_boundary = px.scatter(\n",
    "    boundary_df,\n",
    "    x='distance_to_centroid',\n",
    "    y='cluster',\n",
    "    color='point_type',\n",
    "    hover_name='article_title',\n",
    "    title='Core vs Boundary Articles by Cluster',\n",
    "    labels={'distance_to_centroid': 'Distance to Cluster Center',\n",
    "            'cluster': 'Cluster ID',\n",
    "            'point_type': 'Article Position'},\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "# Display all the visualizations\n",
    "print(\"Distribution of articles across clusters:\")\n",
    "display(cluster_stats)\n",
    "\n",
    "print(\"\\nSample articles from each cluster:\")\n",
    "display(cluster_samples[['cluster', 'article_title']])\n",
    "\n",
    "fig_tsne.show()\n",
    "fig_bar.show()\n",
    "fig_density.show()\n",
    "fig_3d.show()\n",
    "fig_boundary.show()"
   ],
   "id": "18c71b1be5702842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "raw_comments = session.query(\n",
    "    models.PredictedComment.id.label('comment_id'),\n",
    "    models.PredictedComment.article_id.label('article_id'),\n",
    "    models.PredictedComment.text,\n",
    "    models.PredictedComment.text_lang,\n",
    "    models.PredictedComment.ekman_prediction_emotion.label('emotion'),\n",
    "    models.PredictedComment.ekman_prediction_score.label('confidence'),\n",
    "    models.PredictedComment.comment_timestamp,\n",
    "    models.PredictedComment.comment_id,\n",
    ").join(\n",
    "    models.RawArticle,\n",
    "    models.PredictedComment.article_id == models.RawArticle.article_id\n",
    ").filter(\n",
    "    func.cast(models.RawArticle.pub_timestamp, Date) >= '2020-01-01',\n",
    "    models.RawArticle.embedding != None,\n",
    "    models.RawArticle.headline_lang == 'lv',\n",
    "    models.PredictedComment.text_lang == 'lv',\n",
    "    models.PredictedComment.ekman_prediction_emotion != 'neutral',\n",
    ").all()"
   ],
   "id": "ac195f3f4f6e9eb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# convert to dataframe\n",
    "df_comments = pd.DataFrame(raw_comments, columns=['comment_id', 'article_id', 'text', 'text_lang', 'emotion', 'confidence', 'timestamp', 'comment_id'])\n",
    "\n",
    "# assign cluster_id to comments\n",
    "df_comments['cluster_id'] = df_comments['article_id'].map(clustered_articles.set_index('id')['cluster'])\n",
    "\n",
    "# drop comments without cluster_id or with cluster_id -1\n",
    "df_comments = df_comments.dropna(subset=['cluster_id'])\n",
    "df_comments = df_comments[df_comments['cluster_id'] != -1]\n",
    "\n",
    "# add length of comment\n",
    "df_comments['length'] = df_comments['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "df_comments['confidence_cat'] = pd.cut(df_comments['confidence'], bins=[0, 0.33, 0.66, 1], labels=['low', 'medium', 'high'])\n",
    "df_comments['length_cat'] = pd.cut(df_comments['length'], bins=[0, 20, 50, 100, 200], labels=['short', 'medium', 'long', 'very_long'])\n",
    "\n",
    "\n",
    "df_comments"
   ],
   "id": "5650b2ad366116cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sample_with_quota_by_cluster(df_comments, total_samples=2000):\n",
    "    \"\"\"\n",
    "    Sample comments using a straightforward quota approach, working through clusters\n",
    "    until we have the desired number of comments for each emotion.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_comments : pandas DataFrame\n",
    "        Dataframe containing comments with emotion, cluster_id, etc.\n",
    "    total_samples : int\n",
    "        Total number of comments to sample (default: 2000)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Sampled comments with balanced emotion distribution\n",
    "    \"\"\"\n",
    "    # Get unique emotions and set targets\n",
    "    emotions = df_comments['emotion'].unique()\n",
    "    target_per_emotion = total_samples // len(emotions)\n",
    "\n",
    "    # Initialize tracking\n",
    "    collected = {emotion: 0 for emotion in emotions}\n",
    "    final_sample = []\n",
    "\n",
    "    # Get list of clusters, sorted by size (largest first)\n",
    "    cluster_sizes = df_comments.groupby('cluster_id').size().sort_values(ascending=False)\n",
    "    cluster_ids = cluster_sizes.index.tolist()\n",
    "\n",
    "    # Also shuffle to avoid bias if sizes are similar\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(cluster_ids)\n",
    "\n",
    "    # Maximum to take from any one cluster for each emotion\n",
    "    max_per_cluster = 6\n",
    "\n",
    "    # Iterate through clusters\n",
    "    for cluster_id in cluster_ids:\n",
    "        cluster_df = df_comments[df_comments['cluster_id'] == cluster_id]\n",
    "\n",
    "        # For each emotion, take a small sample from this cluster\n",
    "        for emotion in emotions:\n",
    "            # Check if we still need more of this emotion\n",
    "            remaining = target_per_emotion - collected[emotion]\n",
    "            if remaining <= 0:\n",
    "                continue\n",
    "\n",
    "            # Get comments for this emotion in this cluster\n",
    "            emotion_df = cluster_df[cluster_df['emotion'] == emotion]\n",
    "\n",
    "            # Determine how many to take\n",
    "            take_count = min(\n",
    "                remaining,          # Don't exceed what we need\n",
    "                len(emotion_df),    # Don't exceed what's available\n",
    "                max_per_cluster     # Don't take too many from one cluster\n",
    "            )\n",
    "\n",
    "            if take_count > 0:\n",
    "                # If we need to stratify by confidence and length, do it here\n",
    "                if len(emotion_df) > take_count and 'confidence_cat' in emotion_df.columns:\n",
    "                    # Simple stratified sampling\n",
    "                    strata = emotion_df.groupby(['confidence_cat', 'length_cat'], observed=True)\n",
    "                    sampled = pd.DataFrame()\n",
    "\n",
    "                    # Calculate number to take from each stratum\n",
    "                    total_in_strata = sum(len(group) for _, group in strata)\n",
    "\n",
    "                    for (_, _), group in strata:\n",
    "                        if len(group) == 0:\n",
    "                            continue\n",
    "\n",
    "                        # Proportional allocation\n",
    "                        stratum_count = int(np.ceil(take_count * len(group) / total_in_strata))\n",
    "                        stratum_count = min(stratum_count, len(group))\n",
    "\n",
    "                        sampled = pd.concat([sampled, group.sample(stratum_count, random_state=42)])\n",
    "\n",
    "                    # Adjust to exactly take_count\n",
    "                    if len(sampled) > take_count:\n",
    "                        sampled = sampled.sample(take_count, random_state=42)\n",
    "                    elif len(sampled) < take_count and len(emotion_df) > len(sampled):\n",
    "                        # Add more if needed\n",
    "                        additional = emotion_df[~emotion_df.index.isin(sampled.index)]\n",
    "                        additional = additional.sample(min(take_count - len(sampled), len(additional)), random_state=42)\n",
    "                        sampled = pd.concat([sampled, additional])\n",
    "                else:\n",
    "                    # Simple random sampling\n",
    "                    sampled = emotion_df.sample(take_count, random_state=42)\n",
    "\n",
    "                # Add to our collection\n",
    "                final_sample.append(sampled)\n",
    "                collected[emotion] += len(sampled)\n",
    "\n",
    "        # Check if we've hit all our targets\n",
    "        if all(collected[emotion] >= target_per_emotion for emotion in emotions):\n",
    "            break\n",
    "\n",
    "    # Combine all samples and shuffle\n",
    "    final_df = pd.concat(final_sample, ignore_index=True)\n",
    "    final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"Sample summary:\")\n",
    "    print(f\"Total comments: {len(final_df)}\")\n",
    "    for emotion in emotions:\n",
    "        print(f\"{emotion}: {collected[emotion]} comments\")\n",
    "    print(f\"From {final_df['cluster_id'].nunique()} unique clusters\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "result = sample_with_quota_by_cluster(df_comments.copy())"
   ],
   "id": "5a25612d19198c6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result",
   "id": "6981cab1924a41c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# save to csv using absolute path",
   "id": "4cac9ff7e85f3aab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# convert timestampt to date\n",
    "copy_result = result.copy()\n",
    "copy_result['timestamp'] = pd.to_datetime(copy_result['timestamp']).dt.date\n",
    "\n",
    "# group by date\n",
    "grouped = copy_result.groupby(['timestamp', 'emotion']).size().reset_index(name='count')\n",
    "\n",
    "fig = px.line(\n",
    "    grouped,\n",
    "    x='timestamp',\n",
    "    y='count',\n",
    "    color='emotion',\n",
    "    title='Number of Comments per Day by Emotion',\n",
    "    labels={'timestamp': 'Date', 'count': 'Number of Comments'},\n",
    "    markers=True\n",
    ")\n",
    "fig.show()"
   ],
   "id": "f5d3a0a6f023850",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
